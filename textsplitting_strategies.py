# -*- coding: utf-8 -*-
"""Synapse1202

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uz6m5eheZrYxU3dka759Zl-1ax8ASKxp
"""

!pip install langchain-text-splitters langchain_experimental spacy tiktoken sentence-transformers
!python -m spacy download en_core_web_sm

TEXT = '''Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.'''

from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter, TokenTextSplitter,SpacyTextSplitter

#Paragraph Splitter
paragraph_splitter = CharacterTextSplitter()
para = paragraph_splitter.split_text(TEXT)
para

#sententce splitter
sentence_splitter =SpacyTextSplitter(pipeline="en_core_web_sm")
sent = sentence_splitter.split_text(TEXT)
sent

#fixed size chunking0
fixed_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)
fixed = fixed_splitter.split_text(TEXT)
fixed

#slidign window
sliding_window = TokenTextSplitter(
    chunk_size=20,
    chunk_overlap=10,
    encoding_name="cl100k_base"
)
sliding_tokens = sliding_window.split_text(TEXT)
sliding_tokens

#semantic chunking
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
hf = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
semantic_splitter = SemanticChunker(hf)
semantic_chunks = semantic_splitter.split_text(TEXT)
semantic_chunks

#Entity Based Chunking
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp(TEXT)
sent_texts = [s.text.strip() for s in doc.sents if s.text.strip()]
print(sent_texts)
entity_map = {}
for ent in doc.ents:
  entity_map.setdefault(ent.text,set())
print(entity_map)
for s in sent_texts:
  s_doc = nlp(s)
  for ent in s_doc.ents:
    entity_map.setdefault(ent.text,set()).add(s)
entity_chunks=[]
for ent,ss in entity_map.items():
  entity_chunks.append(f"ENTITY :{ent}\n"+"\n".join(f"-{X}" for X in sorted(ss)))

entity_chunks=sent_texts
entity_chunks

from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0,separator='',strip_whitespace=False)
text_splitter.create_documents([TEXT])



"""# Task
Provide a comprehensive explanation of text splitting techniques in AI/ML engineering, particularly for LLMs and RAG systems, discussing trade-offs and use cases for CharacterTextSplitter, RecursiveCharacterTextSplitter, TokenTextSplitter, SpacyTextSplitter, SemanticChunker, and Entity-Based Chunking. Walk through the provided notebook, explaining the purpose of library installations, spaCy model download, `TEXT` variable definition, and providing line-by-line explanations for each text splitter implementation found in cells `KdrPsnmcxCXA`, `MuDtxToxxldP`, `O0dYdsazydSV`, `JfawJylG2lxL`, `io7cJP-vyxi9`, `bZzb0h3qzGqe`, `ctTFcNFzzSb7`, `tAu2ubOH0Qvp`, and `j9wi2xop129d`. Conclude with key takeaways for AI Architecture/ML Engineering students.

## Explain Text Splitting Concepts

### Subtask:
Provide a comprehensive explanation of text splitting techniques and their importance in AI/ML engineering, especially in the context of Large Language Models (LLMs), RAG systems, and data preparation. Discuss the trade-offs and use cases for each splitter type demonstrated in the notebook: CharacterTextSplitter, RecursiveCharacterTextSplitter, TokenTextSplitter, SpacyTextSplitter, SemanticChunker, and the concept of Entity-Based Chunking.

### Importance of Text Splitting in AI/ML Engineering

Text splitting, also known as chunking, is a crucial preprocessing step in natural language processing (NLP) and AI/ML engineering, particularly when working with Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems. Its importance stems from several key factors:

1.  **Context Window Limitations of LLMs**: LLMs have a finite context window, meaning they can only process a limited amount of text at once. Long documents must be broken down into smaller, manageable chunks to fit within these constraints. If the text is too long, it gets truncated, leading to loss of critical information.
2.  **Improved Relevance for RAG Systems**: In RAG systems, text splitting is vital for creating effective retrieval units. When a query is made, the system retrieves relevant chunks of information from a knowledge base. If chunks are too large, irrelevant information might dilute the context. If too small, essential context might be fragmented across multiple chunks, making retrieval less effective.
3.  **Efficiency and Cost**: Processing smaller chunks of text is generally more efficient and less computationally expensive for LLMs. This impacts inference speed and API costs.
4.  **Maintaining Coherence**: The goal of text splitting is not just to break text but to do so in a way that preserves semantic meaning and coherence within each chunk. Poor splitting can lead to chunks that lack context or are grammatically incomplete, hindering the model's understanding.
5.  **Data Preparation**: For training and fine-tuning LLMs, text splitting helps in creating training examples of appropriate length, ensuring that the model learns to process information effectively within its architectural limits.

### Text Splitting Techniques

Here's a breakdown of the text splitting techniques demonstrated, along with their core mechanisms, use cases, and trade-offs:

#### 1. `CharacterTextSplitter`

*   **Core Mechanism**: This is a basic text splitter that splits text based on a specified separator character (defaulting to newline `\n\n`). It aims to keep chunks as close to `chunk_size` as possible, respecting the `separator`. If a chunk is larger than `chunk_size`, it will be split by the separator. If no separator is found, it will split by character.
*   **Use Cases**: Simple documents where clear separators like paragraphs exist. Useful for initial, coarse-grained splitting or when you need strict character limits for chunks.
*   **Trade-offs**:
    *   **Precision**: Low. Can easily break semantic units if the `chunk_size` forces a split in the middle of a sentence or idea, especially if the `separator` is not always present at meaningful boundaries.
    *   **Computational Cost**: Very low.
    *   **Suitability for Different Data Types**: Best for structured text with consistent separators (e.g., Markdown, code). Less suitable for highly unstructured or prose-heavy text where semantic boundaries are not marked by simple characters.

#### 2. `RecursiveCharacterTextSplitter`

*   **Core Mechanism**: This splitter attempts to split documents recursively based on a list of characters. It tries to split by the first character in the list. If that doesn't result in chunks of appropriate size, it tries the second, and so on. Common default separators include `["\n\n", "\n", " ", ""]` (paragraph, newline, space, then character).
*   **Use Cases**: General-purpose text splitting where preserving semantic units is important. Often the default choice for most NLP applications due to its flexibility.
*   **Trade-offs**:
    *   **Precision**: Moderate to High. By trying multiple separators, it increases the likelihood of splitting at semantically meaningful points (e.g., between paragraphs or sentences) before resorting to character-level splitting.
    *   **Computational Cost**: Moderate. Slightly more complex than `CharacterTextSplitter` due to recursive attempts.
    *   **Suitability for Different Data Types**: Highly versatile for various text types, from articles and books to reports, as it adapts to different hierarchical structures of text.

#### 3. `TokenTextSplitter`

*   **Core Mechanism**: Splits text based on a token count rather than character count, using a specified encoding (e.g., `cl100k_base` for OpenAI models, or `p50k_base`). It ensures that chunks adhere to token limits, which is directly relevant to LLM context windows.
*   **Use Cases**: Directly optimizing for LLM input. When precise control over token count per chunk is required, or when working with models that have strict token limits.
*   **Trade-offs**:
    *   **Precision**: Moderate. While it respects token limits, splitting purely by token count can still break sentences or ideas if a natural break point doesn't align with the token limit.
    *   **Computational Cost**: Moderate. Requires tokenization, which adds a processing step compared to character-based splitting.
    *   **Suitability for Different Data Types**: Excellent for all text types where LLM context windows are a primary concern.

#### 4. `SpacyTextSplitter`

*   **Core Mechanism**: Leverages the `spaCy` NLP library to perform sentence-aware splitting. It tokenizes the text and then uses `spaCy`'s sentence boundary detection capabilities to split text into chunks that are typically complete sentences or groups of sentences.
*   **Use Cases**: Applications requiring chunks that are semantically coherent sentences. Ideal for question-answering, summarization, or RAG systems where retrieved chunks should be readable and complete thoughts.
*   **Trade-offs**:
    *   **Precision**: High. By splitting at natural sentence boundaries, it generally produces more coherent and semantically rich chunks than character- or token-based splitters.
    *   **Computational Cost**: Higher. Requires loading a `spaCy` model and performing NLP processing (tokenization, sentence segmentation), which is more resource-intensive than simpler splitters.
    *   **Suitability for Different Data Types**: Very suitable for natural language prose. Less ideal for highly structured text, code, or data where sentence structure is not relevant.

#### 5. `SemanticChunker`

*   **Core Mechanism**: This advanced technique splits text based on the semantic similarity of sentences or segments. It typically uses an embedding model to convert sentences into numerical vectors, then applies an algorithm (e.g., cosine similarity, clustering) to find points where the semantic content significantly changes. Chunks are formed where semantic similarity drops below a certain threshold.
*   **Use Cases**: When the goal is to group together related ideas, even if they span across traditional sentence or paragraph boundaries. Particularly useful for complex documents where topics shift subtly or where logical flow is more important than explicit delimiters.
*   **Trade-offs**:
    *   **Precision**: Very High (semantically). Produces highly coherent chunks based on meaning, which can be extremely beneficial for RAG and complex question-answering.
    *   **Computational Cost**: High. Requires an embedding model (e.g., `HuggingFaceEmbeddings`) to process each sentence/segment, which is computationally intensive and can be slow for very large documents.
    *   **Suitability for Different Data Types**: Excellent for rich, prose-based documents where semantic understanding is paramount. Not suitable for highly structured or short texts where semantic shifts are less pronounced or irrelevant.

#### 6. `Entity-Based Chunking`

*   **Core Mechanism**: This method focuses on identifying and extracting specific entities (e.g., persons, organizations, locations, dates) within the text and then creating chunks centered around these entities. Chunks might include sentences or paragraphs that mention a particular entity, effectively grouping all relevant information about that entity together. This often involves Named Entity Recognition (NER) models.
*   **Use Cases**: Knowledge graph construction, highly specific information retrieval (e.g., "What did John Doe do in 2023?"), document summarization focused on key actors or concepts, or when building context for specific entities.
*   **Trade-offs**:
    *   **Precision**: Very High (entity-specific). Provides highly targeted and relevant chunks for specific entities, making information retrieval precise for entity-related queries.
    *   **Computational Cost**: High. Requires advanced NLP techniques like Named Entity Recognition, which can be computationally expensive and may necessitate specialized models.
    *   **Suitability for Different Data Types**: Highly effective for documents rich in identifiable entities (e.g., news articles, legal documents, biographies, scientific papers). Less useful for abstract or general prose without strong entity references. The output can be a list of sentences related to an entity, or a document where each

## Explain Imports and Setup

### Subtask:
Walk through the initial code cells, explaining the purpose of installing libraries (langchain-text-splitters, spacy, tiktoken, sentence-transformers) and downloading the spaCy model (en_core_web_sm). Also, explain the definition of the TEXT variable.

### Explanation of Initial Setup

**1. Library Installation (`cell_id: KdrPsnmcxCXA`)**

The first code cell installs several essential Python libraries using `!pip install`:

*   `langchain-text-splitters`: This library provides various text splitting methods, which are crucial for breaking down large texts into smaller, manageable chunks for processing by language models.
*   `langchain_experimental`: This package includes experimental features for LangChain, particularly used here for advanced chunking techniques like `SemanticChunker`.
*   `spacy`: A powerful open-source library for advanced Natural Language Processing (NLP). It's used for tokenization, part-of-speech tagging, named entity recognition (NER), and more, which are foundational for certain text splitting strategies.
*   `tiktoken`: This library is developed by OpenAI and is used for token-based text splitting, specifically with OpenAI's encodings, ensuring that text chunks adhere to token limits of models.
*   `sentence-transformers`: This library is used for generating embeddings (numerical representations of text) from sentences or paragraphs. These embeddings are vital for semantic text splitting, where chunks are created based on their semantic similarity.

**2. spaCy Model Download (`cell_id: KdrPsnmcxCXA`)**

The command `!python -m spacy download en_core_web_sm` downloads a small English language model (`en_core_web_sm`) for spaCy. This model is critical for spaCy's NLP functionalities, including tokenization and sentence segmentation. It is specifically utilized by the `SpacyTextSplitter` and for performing entity-based chunking later in the notebook, allowing spaCy to understand the structure and meaning of the English text.

**3. Definition of the `TEXT` Variable (`cell_id: MuDtxToxxldP`)**

The `TEXT` variable is defined as a multiline string containing a sample paragraph about artificial intelligence transforming healthcare. This variable serves as the input text for all subsequent demonstrations of different text splitting techniques in the notebook, allowing us to observe how each method processes the same piece of content.

## Explain CharacterTextSplitter

### Subtask:
Provide a line-by-line explanation of the code using `CharacterTextSplitter` (cell `O0dYdsazydSV` and `JfawJylG2lxL`), detailing how it splits text based on characters and separators, and discussing its application.

### Explain CharacterTextSplitter

#### Instructions

`CharacterTextSplitter` is a basic text splitting method that splits text based on characters. It is often used for simple splitting tasks or as a fallback when more sophisticated methods are not suitable or necessary. Let's break down its usage in the provided cells.

#### Cell `O0dYdsazydSV` Explanation:

```python
#Paragraph Splitter
paragraph_splitter = CharacterTextSplitter()
para = paragraph_splitter.split_text(TEXT)
para
```

1.  **`paragraph_splitter = CharacterTextSplitter()`**:
    *   This line initializes an instance of `CharacterTextSplitter`. When no parameters are explicitly passed during instantiation, `CharacterTextSplitter` uses its default values. The key default values here are `separator='\n\n'` (double newline) and `chunk_size=None` (meaning it won't force chunks by size unless a separator is found).

2.  **`para = paragraph_splitter.split_text(TEXT)`**:
    *   This line calls the `split_text` method on the `paragraph_splitter` instance, passing the `TEXT` variable. The splitter attempts to find the default separator (`\n\n`) within the `TEXT` to divide it into chunks.

3.  **Output: `['Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.']`**:
    *   The output shows a list containing a single string. This occurs because the default separator `\n\n` (double newline) is *not* present in the `TEXT`. Since `chunk_size` was also not specified (it defaults to `None`), the splitter does not have an instruction to break the text by a fixed size. Consequently, the entire input `TEXT` is returned as one large chunk.

#### Cell `JfawJylG2lxL` Explanation:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0,separator='',strip_whitespace=False)
text_splitter.create_documents([TEXT])
```

1.  **`from langchain_text_splitters import CharacterTextSplitter`**:
    *   This line imports the `CharacterTextSplitter` class from the `langchain_text_splitters` library, making it available for use in the code.

2.  **`text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0,separator='',strip_whitespace=False)`**:
    *   This line instantiates `CharacterTextSplitter` with explicit parameters:
        *   `chunk_size=10`: This specifies that each resulting text chunk should ideally have a maximum of 10 characters.
        *   `chunk_overlap=0`: This means there will be no overlapping characters between consecutive chunks.
        *   `separator=''`: This is a crucial parameter. An empty string as a separator tells the splitter to split the text at every single character, effectively breaking the text into individual characters if `chunk_size` were 1. However, combined with `chunk_size`, it instructs the splitter to create chunks of up to `chunk_size` characters, without relying on any specific character for a logical break.
        *   `strip_whitespace=False`: This parameter indicates that leading or trailing whitespace around the chunks should not be removed. Since the separator is an empty string, this parameter's effect is less visible but ensures no implicit removal of spaces if they were to fall at chunk boundaries.

3.  **`text_splitter.create_documents([TEXT])`**:
    *   This method processes the input `TEXT` (provided as a list containing one string) using the configured `text_splitter`. It breaks down the `TEXT` into smaller `Document` objects, where each `Document`'s `page_content` attribute contains a chunk of the original text, based on `chunk_size`, `chunk_overlap`, and `separator` settings.

4.  **Resulting Output**:
    *   The output `[Document(metadata={}, page_content='Artificial'), Document(metadata={}, page_content=' intellege'), ...]` demonstrates that the text has been split into `Document` objects, each containing a chunk of 10 characters (except possibly the last one). For example, `Artificial` is the first 10 characters, ` intellege` is the next 10, and so on. This clearly shows character-by-character splitting into fixed-size chunks, as defined by `chunk_size=10` and `separator=''`. Spaces are treated as regular characters and are included in the chunks.

#### Application and Trade-offs of `CharacterTextSplitter`:

*   **Applications:**
    *   **Simple/Basic Splitting:** When you need a very straightforward splitting mechanism, perhaps for prototyping or when the structure of the text is not critical. It's the most primitive text splitter.
    *   **Fixed-Size Chunking (Character-level):** Useful when you absolutely need chunks of a specific character length, regardless of semantic meaning, as demonstrated in `cell JfawJylG2lxL` with `separator=''`. This can be for certain technical requirements or when character count is the primary constraint.
    *   **Pre-processing for other tasks:** Sometimes used as a first step to break down very large documents into manageable character-bound segments before applying more advanced processing.

*   **Trade-offs/Limitations:**
    *   **Lack of Semantic Coherence:** The primary limitation is that `CharacterTextSplitter` does not consider the linguistic or semantic meaning of the text. It can easily cut words, sentences, or paragraphs in half, leading to semantically incoherent chunks. This is evident in `cell JfawJylG2lxL` where words like 'intelligence' are split into 'Artificia' and 'l intellege'.
    *   **Overly Simplistic:** For most Natural Language Processing (NLP) tasks, breaking text purely by character count or simple separators (like `\n\n`) is often insufficient as it disregards sentence structure, paragraph breaks, and contextual meaning, which are crucial for effective language models.
    *   **Ineffective with complex documents:** It struggles with documents that have varied structures or require more intelligent splitting (e.g., splitting by paragraphs, sections, or based on specific tokens or entities). In such cases, `RecursiveCharacterTextSplitter`, `SpacyTextSplitter`, `TokenTextSplitter`, or `SemanticChunker` are far more appropriate.

## Explain SpacyTextSplitter

### Subtask:
Explain the code for SpacyTextSplitter (cell `io7cJP-vyxi9`), focusing on its use of spaCy's NLP capabilities for sentence-level splitting and the implications for linguistic accuracy.

The `SpacyTextSplitter` from `langchain-text-splitters` is designed to leverage the advanced natural language processing (NLP) capabilities of the spaCy library for intelligent text segmentation. Unlike simpler text splitters that rely on character counts or basic delimiters, `SpacyTextSplitter` uses spaCy's pre-trained language models to understand the linguistic structure of the text, enabling more accurate sentence-level splitting.

### How it works:
When initialized with a `pipeline` (e.g., `"en_core_web_sm"`), `SpacyTextSplitter` loads the specified spaCy model. This model includes components for tokenization, part-of-speech tagging, dependency parsing, and crucially for text splitting, sentence boundary detection (SBD). SpaCy's SBD is rule-based and statistical, making it highly effective at identifying true sentence boundaries, even in complex cases involving abbreviations, quotes, or varied punctuation.

### Line-by-line explanation of code in cell `io7cJP-vyxi9`:

1.  `sentence_splitter = SpacyTextSplitter(pipeline="en_core_web_sm")`
    *   This line initializes an instance of `SpacyTextSplitter`. The crucial part is the `pipeline="en_core_web_sm"` argument. This tells the splitter to load the small English spaCy model (`en_core_web_sm`). This model is equipped with various NLP components, including a sentence segmenter, which is essential for `SpacyTextSplitter` to identify sentence boundaries accurately.

2.  `sent = sentence_splitter.split_text(TEXT)`
    *   Here, the `split_text` method of the initialized `sentence_splitter` object is called, passing the `TEXT` variable (which contains the input string) as its argument. The `SpacyTextSplitter` processes the `TEXT` using the loaded spaCy model to detect sentence boundaries and then splits the text accordingly. The resulting list of sentences is stored in the `sent` variable.

3.  `sent`
    *   This line simply displays the content of the `sent` variable, which is a list of strings, with each string representing a segmented sentence from the original `TEXT`.

### Output of the `SpacyTextSplitter` (cell `io7cJP-vyxi9`)

The output generated by this cell is a list of strings, `sent`, where each string represents a sentence identified by the spaCy model:

```
['Artificial intellegence is transforming healthcare rapidly.\n\nMachine learning models can now detect desease from medical images with remarkable accuray.\n\nThis technology promises to make diagnosis faster and more accesible worldwide.']
```

It's important to note that the output might appear as a single string with `\n\n` separating the sentences, which is how the `split_text` method of `SpacyTextSplitter` formats its output internally before returning it as a list. The `

` are essentially placeholders that indicate the original structure when the text is concatenated. However, the Python representation `['Sentence 1.\n\nSentence 2.']` means there's a *single string* in the list, and the `\n\n` are part of that string's content. This differs from other splitters that might return `['Sentence 1.', 'Sentence 2.']` as distinct list elements.

Looking at the `sent` variable from the kernel state, it is indeed `['Artificial intellegence is transforming healthcare rapidly.\n\nMachine learning models ...nosis faster and more accesible worldwide.']`. This indicates that `SpacyTextSplitter` by default might not always return perfectly separate strings for each sentence, but rather a single string with internal sentence breaks represented by `\n\n` in the original text structure, or it might be a simplification in how the `split_text` method handles and returns the segmented parts, preserving some original formatting.

The spaCy model's role is to accurately *identify* sentence boundaries. The `SpacyTextSplitter` then uses these identified boundaries to break down the `TEXT`. The `\n\n` characters observed in the output likely originate from how the `SpacyTextSplitter` reconstructs the chunks, maintaining a visual separation that might reflect paragraph breaks or simply how the split is internally represented.

### Advantages of `SpacyTextSplitter` over simpler splitters:

Using `SpacyTextSplitter` offers significant advantages, especially when linguistic accuracy and semantic coherence are critical:

1.  **Linguistic Accuracy in Sentence Boundary Detection:** Simpler character-based splitters often rely on punctuation marks like periods, question marks, and exclamation points to identify sentence endings. However, this approach can be flawed. For instance, periods can also signify abbreviations (e.g., "Dr.", "U.S.") or decimals, which should not break a sentence. `SpacyTextSplitter`, by leveraging spaCy's sophisticated NLP models, can differentiate these cases, leading to much more accurate sentence segmentation. It understands the grammatical structure and context, ensuring that sentences are not prematurely split or incorrectly merged.

2.  **Improved Semantic Coherence:** When text is split into chunks, it's crucial that each chunk maintains semantic integrity. Accurate sentence splitting helps ensure that each generated text segment represents a complete thought or idea. Inaccurate splitting can lead to fragmented sentences, losing context and making downstream tasks (like information retrieval, summarization, or question answering) less effective. `SpacyTextSplitter` produces chunks that are linguistically sound, thus inherently more semantically coherent.

3.  **Handling Complex Text Structures:** SpaCy models are trained on large corpora and can handle a variety of complex sentence structures, including those with quotes, parentheticals, and varied punctuation, where a simple splitter would likely fail. This makes `SpacyTextSplitter` more robust for real-world, diverse textual data.

4.  **Foundation for Further NLP Tasks:** Because `SpacyTextSplitter` relies on a full spaCy pipeline, the underlying linguistic annotations (like part-of-speech tags or named entities) could potentially be leveraged for more advanced chunking strategies or for enriching the generated text segments, although the `SpacyTextSplitter` itself primarily focuses on SBD.

### Trade-offs and Considerations when using `SpacyTextSplitter`:

While `SpacyTextSplitter` offers enhanced accuracy and linguistic intelligence, it's important to consider some potential trade-offs:

1.  **Computational Overhead:** Loading and utilizing a spaCy model (even a small one like `en_core_web_sm`) introduces a higher computational cost compared to basic string manipulation or regex-based splitters. This can be a factor for very large datasets or applications with strict latency requirements. The initialization of the `SpacyTextSplitter` involves loading the entire spaCy pipeline, which consumes memory and processing power.

2.  **Dependency on spaCy Models:** The `SpacyTextSplitter` inherently depends on spaCy and its pre-trained models. This means you need to have spaCy installed and the relevant model downloaded. While `en_core_web_sm` is relatively small, other larger models might consume more disk space and load time. If the target language is not well-supported by spaCy, the accuracy of the splitter might be compromised.

3.  **Potential for Errors in Specific Domains:** While generally robust, spaCy models are trained on general-purpose text. In highly specialized domains (e.g., medical reports, legal documents), sentence structures or terminology might deviate significantly from common usage, potentially leading to less accurate sentence boundary detection. Fine-tuning spaCy models or using domain-specific rule sets might be necessary in such cases.

4.  **Version Compatibility:** As `SpacyTextSplitter` relies on `langchain-text-splitters` and `spaCy`, compatibility between different versions of these libraries needs to be managed to avoid unexpected behavior or errors.

### Trade-offs and Considerations when using `SpacyTextSplitter`:

While `SpacyTextSplitter` offers enhanced accuracy and linguistic intelligence, it's important to consider some potential trade-offs:

1.  **Computational Overhead:** Loading and utilizing a spaCy model (even a small one like `en_core_web_sm`) introduces a higher computational cost compared to basic string manipulation or regex-based splitters. This can be a factor for very large datasets or applications with strict latency requirements. The initialization of the `SpacyTextSplitter` involves loading the entire spaCy pipeline, which consumes memory and processing power.

2.  **Dependency on spaCy Models:** The `SpacyTextSplitter` inherently depends on spaCy and its pre-trained models. This means you need to have spaCy installed and the relevant model downloaded. While `en_core_web_sm` is relatively small, other larger models might consume more disk space and load time. If the target language is not well-supported by spaCy, the accuracy of the splitter might be compromised.

3.  **Potential for Errors in Specific Domains:** While generally robust, spaCy models are trained on general-purpose text. In highly specialized domains (e.g., medical reports, legal documents), sentence structures or terminology might deviate significantly from common usage, potentially leading to less accurate sentence boundary detection. Fine-tuning spaCy models or using domain-specific rule sets might be necessary in such cases.

4.  **Version Compatibility:** As `SpacyTextSplitter` relies on `langchain-text-splitters` and `spaCy`, compatibility between different versions of these libraries needs to be managed to avoid unexpected behavior or errors.

## Explain RecursiveCharacterTextSplitter

### Subtask:
Detail the functionality of `RecursiveCharacterTextSplitter` (cell `bZzb0h3qzGqe`), explaining how `chunk_size` and `chunk_overlap` work and the benefit of using multiple separators for more robust splitting.

The `RecursiveCharacterTextSplitter` is a powerful and flexible text splitter designed to maintain semantic coherence as much as possible by attempting to split text using a list of separators in a hierarchical manner. Instead of splitting text based on a single character or a fixed token count, it tries various separators in order, working its way down from larger, more semantically significant units (like paragraphs) to smaller ones (like words).

### Explanation of `RecursiveCharacterTextSplitter` (Cell `bZzb0h3qzGqe`):

```python
#fixed size chunking0
fixed_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)
fixed = fixed_splitter.split_text(TEXT)
fixed
```

*   `fixed_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)`:
    *   `chunk_size=100`: This parameter specifies the maximum desired size of each chunk, measured in characters. The splitter aims to keep chunks at or below this size. If a piece of text is larger than `chunk_size`, the splitter will attempt to break it down further using its predefined separators.
    *   `chunk_overlap=20`: This parameter defines the number of characters that will overlap between consecutive chunks. Overlap is crucial for preserving context, especially when a key piece of information or a sentence spans across a chunk boundary. By including the last 20 characters of the previous chunk at the beginning of the next, it helps ensure that language models don't lose context between chunks.
    *   The `RecursiveCharacterTextSplitter` internally uses a default list of separators (e.g., `["\n\n", "\n", " ", ""]`). It first tries to split by the first separator. If the resulting chunks are too large, it recursively applies the next separator to those chunks, and so on, until all chunks are within the `chunk_size` limit or it has exhausted all separators.

*   `fixed = fixed_splitter.split_text(TEXT)`:
    *   This line invokes the `split_text` method of the `fixed_splitter` instance. It takes the input `TEXT` string and applies the recursive splitting logic with the specified `chunk_size` and `chunk_overlap` to divide the text into smaller, manageable pieces.

*   `fixed`:
    *   This line simply displays the `fixed` variable, which holds the list of text chunks generated by the splitter. The output shows the individual text segments after the splitting process.

### Analysis of the Output from Cell `bZzb0h3qzGqe`:

```
['Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect',
 'can now detect desease from medical images with remarkable accuray.This technology promises to make',
 'promises to make diagnosis faster and more accesible worldwide.']
```

The original `TEXT` is:
`'Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.'`

1.  **First chunk:** `'Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect'` (94 characters). This chunk is close to the `chunk_size` of 100. The splitter likely found a suitable breaking point (possibly a space or implicit sentence boundary) after 'detect'.
2.  **Second chunk:** `'can now detect desease from medical images with remarkable accuray.This technology promises to make'` (97 characters). Notice the `chunk_overlap`. The phrase "can now detect" (14 characters + 1 space from `detect`) is present in both the first and second chunks. This overlap helps to maintain continuity. The splitter aims to break before a new sentence or logical unit.
3.  **Third chunk:** `'promises to make diagnosis faster and more accesible worldwide.'` (65 characters). Again, observe the overlap: "promises to make" (16 characters) from the end of the second chunk appears at the start of the third. This chunk contains the remaining text.

The splitter successfully divided the text into chunks of approximately 100 characters while incorporating the specified overlap.

### Benefits of `RecursiveCharacterTextSplitter`:

Compared to `CharacterTextSplitter`, which often relies on a single separator (like a newline) or a very simple fixed-character split, `RecursiveCharacterTextSplitter` offers significant advantages:

*   **Semantic Coherence:** By attempting to split on larger, more natural boundaries first (e.g., paragraphs, sentences), it prioritizes keeping related information together. This leads to more meaningful chunks that are easier for language models to process and understand.
*   **Robustness:** The use of multiple separators makes it more robust to varied text formats. If one separator doesn't yield suitable chunk sizes, it tries the next, ensuring that text is always split, even if it means resorting to splitting by individual characters or falling back to a character-level split.
*   **Adaptability:** It adapts better to different text structures. For example, if a document has well-defined sections, it will try to respect those. If not, it will default to splitting by words or characters.

### Trade-offs:

While highly beneficial, `RecursiveCharacterTextSplitter` also has some trade-offs:

*   **Potential for Arbitrary Splits:** If a `chunk_size` is very small or if the text contains very long, unbroken sequences of characters without any of the specified separators, the splitter may eventually have to cut in the middle of a word or a sentence. This can lead to less semantically coherent chunks, although it's less common than with simpler splitters.
*   **Context Loss Mitigation vs. Redundancy:** `chunk_overlap` is excellent for mitigating context loss across chunk boundaries, as it provides a 'window' into the previous chunk. However, this also introduces **redundancy** because some text is duplicated across chunks. While beneficial for understanding, it means more tokens are processed overall, which can impact processing time and cost for large documents.

## Explain TokenTextSplitter

### Subtask:
Provide a line-by-line explanation of TokenTextSplitter (cell `ctTFcNFzzSb7`), emphasizing tokenization using `tiktoken` encoding and its relevance for managing LLM context windows.

## Explanation of `TokenTextSplitter`

### 1. Core Concept of `TokenTextSplitter`

The `TokenTextSplitter` is a crucial tool in Natural Language Processing (NLP), especially when working with Large Language Models (LLMs). Unlike character-based text splitters, it divides text into smaller `chunks` based on _tokens_ rather than raw characters. Tokenization is the process of breaking down text into individual units (tokens), which can be words, subwords, or even characters, depending on the tokenizer. For LLMs, token-based splitting is vital because LLMs process text in terms of tokens, and they have a strict `context window` (a maximum number of tokens they can process at once). Efficiently splitting text by tokens helps manage this context window, ensuring that chunks are within the LLM's limit and maintain semantic coherence.

### 2. Line-by-Line Explanation of Cell `ctTFcNFzzSb7`

```python
#slidign window
sliding_window = TokenTextSplitter(
    chunk_size=20,
    chunk_overlap=10,
    encoding_name="cl100k_base"
)
sliding_tokens = sliding_window.split_text(TEXT)
sliding_tokens
```

*   `sliding_window = TokenTextSplitter(...)`: This line instantiates the `TokenTextSplitter` object. It's configured with three key parameters:
    *   `chunk_size=20`: This specifies the maximum number of tokens each chunk should contain. In this case, each resulting text segment will attempt to be 20 tokens long.
    *   `chunk_overlap=10`: This defines the number of tokens that will overlap between consecutive `chunks`. An overlap helps maintain context across `chunks`, preventing loss of meaning at the boundaries. Here, the last 10 tokens of one `chunk` will be the first 10 tokens of the next `chunk`.
    *   `encoding_name="cl100k_base"`: This is a critical parameter. It specifies the `tiktoken` encoding to be used for tokenization. `"cl100k_base"` is the encoding used by many modern OpenAI models (like GPT-3.5 and GPT-4). By specifying this, the `TokenTextSplitter` uses the same tokenization rules as the target LLM, ensuring accurate token counts and chunk sizes that align with the LLM's `context window`.

*   `sliding_tokens = sliding_window.split_text(TEXT)`: This line calls the `split_text()` method on the `TokenTextSplitter` instance (`sliding_window`), passing the `TEXT` variable (which contains our sample text) as input. This method performs the actual token-based splitting according to the configured `chunk_size` and `chunk_overlap`.

*   `sliding_tokens`: This line simply prints the resulting list of text `chunks` to the console.

### 3. Analysis of the Output

The output of cell `ctTFcNFzzSb7` is:

```
['Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical',
 'achine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make',
 ' images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.']
```

Notice that the `TEXT` was split based on tokens, not simply characters or arbitrary lines. Each chunk contains approximately 20 tokens (considering the `chunk_overlap`). For example, the first chunk ends with "medical", and the second chunk starts with "achine learning models can now detect desease from medical", clearly demonstrating the `chunk_overlap` of 10 tokens. The `tiktoken` encoding treats sub-word units as tokens, which is why words like "rapidly.Machine" are split or re-contextualized to fit the token count. This precise token-based splitting ensures that when these `chunks` are fed into an LLM, they respect its `context window` limit and allow for better contextual understanding due to the overlap.

### 4. Advantages of `TokenTextSplitter` for LLM Context Windows and Cost Efficiency

*   **Precise Context Window Management**: LLMs have strict token limits. `TokenTextSplitter` allows developers to precisely control the number of tokens in each `chunk`, preventing `chunks` from exceeding the LLM's `context window`. This is crucial for avoiding truncation of input and ensuring that the LLM receives all necessary information.
*   **Semantic Coherence**: By using `chunk_overlap`, `TokenTextSplitter` helps maintain the semantic flow between `chunks`. This overlap provides the LLM with sufficient context from the previous `chunk`, which is essential for tasks like summarization, Q&A, or document analysis where information often spans across multiple `chunks`.
*   **Cost Efficiency**: Most LLM APIs charge based on token usage. By accurately splitting text into optimal `chunk` sizes, `TokenTextSplitter` helps avoid sending unnecessarily long or redundant `chunks` to the LLM, leading to more efficient token usage and reduced API costs.
*   **Compatibility with LLMs**: Using an `encoding_name` like `"cl100k_base"` ensures that the text is tokenized in the same way the LLM expects, preventing discrepancies that could lead to unexpected `chunk` sizes or misinterpreted inputs.

### 5. Trade-offs or Limitations of `TokenTextSplitter`

While highly advantageous, `TokenTextSplitter` is not without its trade-offs:

*   **Potential for Semantic Breaks**: Despite `chunk_overlap`, if `chunk_size` is too small or the text has very long, complex sentences, a `chunk` might still end in the middle of a sentence or a semantically important phrase. This can lead to fragmented information and a loss of complete meaning within a single `chunk`, potentially impacting the LLM's ability to fully understand the context.
*   **Complexity with `tiktoken`**: Relying on specific `tiktoken` encodings can sometimes be a black box if the user is not familiar with how different tokenizers split words. This can make it challenging to predict exactly where `chunks` will break, especially with diverse or unconventional text.
*   **Overlapping Redundancy**: While `chunk_overlap` is beneficial for context, excessive overlap can lead to redundant information being processed by the LLM, potentially increasing token usage (and cost) without a proportional gain in performance.
*   **Configuration Dependency**: The effectiveness of `TokenTextSplitter` heavily depends on the correct configuration of `chunk_size` and `chunk_overlap`. Finding the optimal values often requires experimentation and understanding of both the text content and the LLM's capabilities.

## Explanation of `TokenTextSplitter`

### 1. Core Concept of `TokenTextSplitter`

The `TokenTextSplitter` is a crucial tool in Natural Language Processing (NLP), especially when working with Large Language Models (LLMs). Unlike character-based text splitters, it divides text into smaller `chunks` based on _tokens_ rather than raw characters. Tokenization is the process of breaking down text into individual units (tokens), which can be words, subwords, or even characters, depending on the tokenizer. For LLMs, token-based splitting is vital because LLMs process text in terms of tokens, and they have a strict `context window` (a maximum number of tokens they can process at once). Efficiently splitting text by tokens helps manage this context window, ensuring that chunks are within the LLM's limit and maintain semantic coherence.

### 2. Line-by-Line Explanation of Cell `ctTFcNFzzSb7`

```python
#slidign window
sliding_window = TokenTextSplitter(
    chunk_size=20,
    chunk_overlap=10,
    encoding_name="cl100k_base"
)
sliding_tokens = sliding_window.split_text(TEXT)
sliding_tokens
```

*   `sliding_window = TokenTextSplitter(...)`: This line instantiates the `TokenTextSplitter` object. It's configured with three key parameters:
    *   `chunk_size=20`: This specifies the maximum number of tokens each chunk should contain. In this case, each resulting text segment will attempt to be 20 tokens long.
    *   `chunk_overlap=10`: This defines the number of tokens that will overlap between consecutive `chunks`. An overlap helps maintain context across `chunks`, preventing loss of meaning at the boundaries. Here, the last 10 tokens of one `chunk` will be the first 10 tokens of the next `chunk`.
    *   `encoding_name="cl100k_base"`: This is a critical parameter. It specifies the `tiktoken` encoding to be used for tokenization. `"cl100k_base"` is the encoding used by many modern OpenAI models (like GPT-3.5 and GPT-4). By specifying this, the `TokenTextSplitter` uses the same tokenization rules as the target LLM, ensuring accurate token counts and chunk sizes that align with the LLM's `context window`.

*   `sliding_tokens = sliding_window.split_text(TEXT)`: This line calls the `split_text()` method on the `TokenTextSplitter` instance (`sliding_window`), passing the `TEXT` variable (which contains our sample text) as input. This method performs the actual token-based splitting according to the configured `chunk_size` and `chunk_overlap`.

*   `sliding_tokens`: This line simply prints the resulting list of text `chunks` to the console.

### 3. Analysis of the Output

The output of cell `ctTFcNFzzSb7` is:

```
['Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical',
 'achine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make',
 ' images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.']
```

Notice that the `TEXT` was split based on tokens, not simply characters or arbitrary lines. Each chunk contains approximately 20 tokens (considering the `chunk_overlap`). For example, the first chunk ends with "medical", and the second chunk starts with "achine learning models can now detect desease from medical", clearly demonstrating the `chunk_overlap` of 10 tokens. The `tiktoken` encoding treats sub-word units as tokens, which is why words like "rapidly.Machine" are split or re-contextualized to fit the token count. This precise token-based splitting ensures that when these `chunks` are fed into an LLM, they respect its `context window` limit and allow for better contextual understanding due to the overlap.

### 4. Advantages of `TokenTextSplitter` for LLM Context Windows and Cost Efficiency

*   **Precise Context Window Management**: LLMs have strict token limits. `TokenTextSplitter` allows developers to precisely control the number of tokens in each `chunk`, preventing `chunks` from exceeding the LLM's `context window`. This is crucial for avoiding truncation of input and ensuring that the LLM receives all necessary information.
*   **Semantic Coherence**: By using `chunk_overlap`, `TokenTextSplitter` helps maintain the semantic flow between `chunks`. This overlap provides the LLM with sufficient context from the previous `chunk`, which is essential for tasks like summarization, Q&A, or document analysis where information often spans across multiple `chunks`.
*   **Cost Efficiency**: Most LLM APIs charge based on token usage. By accurately splitting text into optimal `chunk` sizes, `TokenTextSplitter` helps avoid sending unnecessarily long or redundant `chunks` to the LLM, leading to more efficient token usage and reduced API costs.
*   **Compatibility with LLMs**: Using an `encoding_name` like `"cl100k_base"` ensures that the text is tokenized in the same way the LLM expects, preventing discrepancies that could lead to unexpected `chunk` sizes or misinterpreted inputs.

### 5. Trade-offs or Limitations of `TokenTextSplitter`

While highly advantageous, `TokenTextSplitter` is not without its trade-offs:

*   **Potential for Semantic Breaks**: Despite `chunk_overlap`, if `chunk_size` is too small or the text has very long, complex sentences, a `chunk` might still end in the middle of a sentence or a semantically important phrase. This can lead to fragmented information and a loss of complete meaning within a single `chunk`, potentially impacting the LLM's ability to fully understand the context.
*   **Complexity with `tiktoken`**: Relying on specific `tiktoken` encodings can sometimes be a black box if the user is not familiar with how different tokenizers split words. This can make it challenging to predict exactly where `chunks` will break, especially with diverse or unconventional text.
*   **Overlapping Redundancy**: While `chunk_overlap` is beneficial for context, excessive overlap can lead to redundant information being processed by the LLM, potentially increasing token usage (and cost) without a proportional gain in performance.
*   **Configuration Dependency**: The effectiveness of `TokenTextSplitter` heavily depends on the correct configuration of `chunk_size` and `chunk_overlap`. Finding the optimal values often requires experimentation and understanding of both the text content and the LLM's capabilities.

## Explain SemanticChunker

### Subtask:
Explain the `SemanticChunker` (cell `tAu2ubOH0Qvp`) from `langchain_experimental`, including the role of `HuggingFaceEmbeddings` in identifying semantically similar text for chunking and its advanced use cases.

### 1. Core Concept of `SemanticChunker`

`SemanticChunker` is a text splitting tool from `langchain_experimental` that aims to split text into chunks based on semantic meaning rather than arbitrary fixed lengths or character delimiters. Unlike traditional text splitters that might break text at a certain character count or specific separators, `SemanticChunker` leverages embedding models to understand the content's meaning.

The process works by:
1. **Generating Embeddings**: It converts sentences or small segments of the text into numerical vectors (embeddings) using an embedding model. These embeddings capture the semantic meaning of the text, with semantically similar texts having closer vectors in the embedding space.
2. **Identifying Semantic Boundaries**: It then analyzes the differences between consecutive sentence embeddings. When the semantic similarity between two adjacent sentences drops below a certain threshold (or conversely, the difference/distance exceeds a threshold), it indicates a potential change in topic or context. This point is considered a semantic boundary.
3. **Chunking**: The text is then split at these identified semantic boundaries, resulting in chunks that are more coherent and self-contained in terms of meaning.

### 2. Line-by-Line Explanation of the Code in Cell `tAu2ubOH0Qvp`

```python
#semantic chunking
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
hf = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
semantic_splitter = SemanticChunker(hf)
semantic_chunks = semantic_splitter.split_text(TEXT)
semantic_chunks
```

1.  **`from langchain_experimental.text_splitter import SemanticChunker`**: This line imports the `SemanticChunker` class, which is responsible for splitting text based on semantic similarity, from the `langchain_experimental` library.

2.  **`from langchain_community.embeddings import HuggingFaceEmbeddings`**: This line imports the `HuggingFaceEmbeddings` class, a specific implementation of an embedding model that leverages models available on Hugging Face, from the `langchain_community` library.

3.  **`hf = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")`**: Here, an instance of `HuggingFaceEmbeddings` is created. The `model_name` parameter specifies which pre-trained sentence transformer model to use for generating embeddings. `"sentence-transformers/all-MiniLM-L6-v2"` is a popular, efficient model known for producing good quality embeddings for various text tasks. This `hf` object will be used by the `SemanticChunker` to convert text segments into their numerical vector representations.

4.  **`semantic_splitter = SemanticChunker(hf)`**: This line instantiates the `SemanticChunker`. It takes the `hf` embedding model instance as an argument. This tells the `SemanticChunker` to use the `all-MiniLM-L6-v2` model for all its embedding generation needs when determining semantic boundaries.

5.  **`semantic_chunks = semantic_splitter.split_text(TEXT)`**: This is the core operation where the `TEXT` string (defined previously) is passed to the `split_text` method of the `semantic_splitter`. The `SemanticChunker` internally processes the text, generates embeddings for its constituent sentences/segments, identifies semantic boundaries using the embedding model, and then splits the text accordingly. The resulting chunks are stored in the `semantic_chunks` list.

6.  **`semantic_chunks`**: This line simply prints the `semantic_chunks` list to display the output of the text splitting process.

### 3. Analysis of the Output from Cell `tAu2ubOH0Qvp`

The output from cell `tAu2ubOH0Qvp` is:

```
['Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.']
```

This output indicates that the `SemanticChunker` kept the entire input `TEXT` as a single chunk. The `TEXT` provided is:

`'Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.'`

The reason it remained a single chunk is likely due to the inherent semantic coherence of the input text. The entire paragraph discusses a singular topic: the impact of artificial intelligence and machine learning on healthcare, specifically regarding disease detection and diagnosis. The sentences flow logically and maintain a consistent semantic theme throughout.

The `SemanticChunker`, by design, looks for significant semantic shifts or topic changes to identify chunk boundaries. In this particular short and focused text, the embedding model (`all-MiniLM-L6-v2`) likely did not detect a large enough semantic distance between consecutive sentences to warrant splitting them. Consequently, it determined that the entire text represented a single, cohesive semantic unit and therefore returned it as one chunk. If the text were longer and covered multiple distinct topics, we would expect the `SemanticChunker` to produce multiple, semantically distinct chunks.

### 4. Advanced Use Cases of `SemanticChunker`

`SemanticChunker` offers significant advantages over traditional text splitting methods, particularly in applications where semantic coherence is paramount:

*   **Improved Retrieval Augmented Generation (RAG)**: In RAG systems, the quality of retrieved chunks directly impacts the generated response. By ensuring that each chunk is semantically coherent and covers a distinct topic, `SemanticChunker` helps retrieve more relevant and contextually rich information, leading to more accurate and helpful LLM responses. Fixed-size chunks often cut off ideas mid-sentence or combine unrelated topics, diluting the retrieved context.

*   **Better Context Preservation for Complex Documents**: For lengthy and intricate documents (e.g., research papers, legal documents, technical manuals), maintaining topic integrity within chunks is crucial. `SemanticChunker` can segment these documents into logical units, making it easier for downstream models or human readers to understand the flow of information without losing critical context across chunk boundaries.

*   **Enhanced Summarization and Question Answering**: When summarizing large texts or answering questions that require deep understanding, having semantically meaningful chunks means that each chunk can be treated as a self-contained unit of information. This can simplify the process of identifying key points or extracting specific answers, as the LLM doesn't have to piece together fragmented ideas from multiple, arbitrarily split chunks.

*   **Reduced Hallucinations**: By providing more coherent and complete contextual chunks to an LLM, the risk of the model hallucinating or generating irrelevant information is significantly reduced. The LLM receives clear, topic-specific information, enabling it to stay grounded in the provided context.

### 5. Trade-offs of `SemanticChunker`

While `SemanticChunker` offers significant advantages, it also comes with certain trade-offs that need to be considered:

*   **Computational Cost of Embedding Generation**: The primary trade-off is the computational overhead. Unlike character-based or token-based splitters that perform simple string operations, `SemanticChunker` requires the generation of embeddings for segments of the text (typically sentences). This involves running a deep learning model (`HuggingFaceEmbeddings` in this case) for inference, which can be computationally intensive and time-consuming, especially for very large documents or high-throughput applications. The cost scales with the size of the document and the complexity of the embedding model.

*   **Dependency on External Embedding Models**: `SemanticChunker` is entirely dependent on the quality and availability of the chosen embedding model. The effectiveness of the semantic splitting relies directly on how well the embedding model captures the semantic nuances of the text. If the embedding model is not suitable for the specific domain or language of the text, the resulting chunks may not be semantically coherent. Additionally, using hosted embedding models might incur API costs and introduce latency.

*   **Potential for Misinterpretation**: While generally more robust than fixed-size chunking, the semantic interpretation is not always perfect. The embedding model might occasionally misinterpret semantic boundaries, leading to chunks that are either too short (splitting a coherent thought) or too long (combining distinct topics that the model didn't differentiate).

*   **Increased Resource Usage**: Loading and running embedding models, especially larger ones, requires more memory (RAM) and potentially GPU resources, which might be a constraint in certain environments or for edge deployments.

## Explain Entity-Based Chunking

### Subtask:
Clarify the concept and implementation of Entity-Based Chunking (cell `j9wi2xop129d`), discussing how it uses spaCy to identify entities and group related sentences, and its potential for improving retrieval in RAG systems.

## Explain Entity-Based Chunking

### Core Concept of Entity-Based Chunking

Entity-Based Chunking is a text splitting strategy that focuses on preserving the context around specific entities (persons, organizations, locations, dates, etc.) within a document. Instead of splitting text purely by character count or predefined delimiters, it leverages **Named Entity Recognition (NER)** to identify important entities. The goal is to group sentences or text segments that mention or are highly relevant to a particular entity together. This approach aims to create more semantically coherent chunks, making them more useful for retrieval tasks where the user's query might revolve around specific entities.

### Line-by-Line Explanation of Code in Cell `j9wi2xop129d`

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp(TEXT)
sent_texts = [s.text.strip() for s in doc.sents if s.text.strip()]
print(entity_map)
for ent in doc.ents:
  entity_map.setdefault(ent.text,set())
print(entity_map)
for s in sent_texts:
  s_doc = nlp(s)
  for ent in s_doc.ents:
    entity_map.setdefault(ent.text,set()).add(s)
entity_chunks=[]
for ent,ss in entity_map.items():
  entity_chunks.append(f"ENTITY :{ent}\n"+"\n".join(f"-{X}" for X in sorted(ss)))
entity_chunks=sent_texts
entity_chunks
```

1.  **`import spacy`**: This line imports the spaCy library, a popular open-source library for advanced Natural Language Processing (NLP) in Python.

2.  **`nlp = spacy.load("en_core_web_sm")`**: This loads a pre-trained English language model (`en_core_web_sm`) from spaCy. This model includes components for tokenization, part-of-speech tagging, dependency parsing, and, crucially for this task, **Named Entity Recognition (NER)**. It also provides capabilities for sentence segmentation.

3.  **`doc = nlp(TEXT)`**: The input `TEXT` is processed by the loaded spaCy `nlp` pipeline. This creates a `Doc` object, which contains linguistic annotations for the text, including identified entities and sentence boundaries.

4.  **`sent_texts = [s.text.strip() for s in doc.sents if s.text.strip()]`**: This line extracts individual sentences from the `doc` object. `doc.sents` is an iterator that yields `Span` objects representing sentences. For each sentence `s`, `s.text.strip()` gets its string content and removes leading/trailing whitespace. The list comprehension constructs `sent_texts`, a list of clean, individual sentences.

5.  **`entity_map = {}`**: An empty dictionary named `entity_map` is initialized. This dictionary is intended to store entities as keys and a set of sentences associated with each entity as values.

6.  **`for ent in doc.ents: entity_map.setdefault(ent.text,set())`**: This loop iterates through all the entities (`doc.ents`) that spaCy's NER component found in the entire `TEXT`. For each entity `ent`, its text (`ent.text`) is used as a key in `entity_map`. `setdefault(key, default_value)` ensures that if the key doesn't exist, it's added with an empty set as its value. This step initializes entries in `entity_map` for all unique entities found.

7.  **`for s in sent_texts: s_doc = nlp(s); for ent in s_doc.ents: entity_map.setdefault(ent.text,set()).add(s)`**: This nested loop is designed to associate sentences with the entities they contain. It iterates through each sentence in `sent_texts`. For each sentence `s`, it's re-processed by `nlp(s)` to create a new `Doc` object (`s_doc`). This is done to ensure entity detection is performed specifically on the sentence boundaries, which can sometimes differ from how entities are detected across the entire document. Then, it iterates through entities found within *that specific sentence* (`s_doc.ents`), and adds the current sentence `s` to the set associated with that entity in `entity_map`.

8.  **`entity_chunks=[]
for ent,ss in entity_map.items():
  entity_chunks.append(f"ENTITY :{ent}\n"+"\n".join(f"-{X}" for X in sorted(ss)))`**: This loop aims to construct the final `entity_chunks`. It iterates through the `entity_map` (key-value pairs of entity text and their associated sentences). For each entity, it formats a string that starts with "ENTITY :[entity_text]" followed by a newline and then lists all the sentences associated with that entity, each prefixed with a hyphen.

9.  **`entity_chunks=sent_texts`**: This line **overwrites** the `entity_chunks` list created in the previous step. It sets `entity_chunks` to be identical to the original list of `sent_texts`. This means the entity-based chunking logic's output, as initially intended in the previous loop, is effectively discarded in the final output of the cell.

#### Analysis of Observed Output

*   **`print(entity_map)` being empty**: After the initial `for ent in doc.ents: entity_map.setdefault(ent.text,set())` loop, `entity_map` is printed as `{}`. This indicates that the `en_core_web_sm` model did not detect any named entities in the specific `TEXT` provided: `'Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.'`. The 'sm' (small) model might have limited entity recognition capabilities, or the text simply lacks entities that it's trained to recognize (like specific people, organizations, locations). Words like 'Artificial intellegence', 'healthcare', 'Machine learning models' are generally not classified as named entities by default 'en_core_web_sm' model, which focuses on proper nouns like names, places, dates, etc.

*   **`entity_chunks` being the same as `sent_texts`**: Due to the fact that no entities were detected, `entity_map` remains empty throughout the process. Consequently, the loop intended to construct `entity_chunks` based on `entity_map` would not add any items. However, the line `entity_chunks=sent_texts` explicitly assigns the list of raw sentences to `entity_chunks`, effectively bypassing any entity-based processing that might have occurred if entities were found.

### General Applications and Potential Benefits for LLMs and RAG Systems

Entity-Based Chunking offers several advantages for Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems:

*   **Improved Contextual Relevance**: By grouping text segments around key entities, RAG systems can retrieve more focused and contextually rich information when a query pertains to a specific entity. For example, if a query is about 'Apple Inc.', chunks related to 'Apple Inc.' as an organization, its products, and its history will be prioritized, ensuring all relevant details are present.
*   **Reduced Irrelevant Information**: Traditional fixed-size chunking might break an entity's context across multiple chunks or include unrelated information within a chunk. Entity-based chunking aims to minimize this by ensuring that all pertinent information about an entity resides within a single or closely related set of chunks.
*   **Enhanced Retrieval Accuracy**: When chunks are semantically aligned with entities, the embeddings generated for these chunks can better capture the entity's meaning. This leads to more accurate retrieval of relevant documents or passages, as the similarity between the query's embedding and the chunk's embedding will be higher for entity-centric queries.
*   **Better Summarization and Generation**: For LLMs, having well-formed, entity-specific chunks as context can lead to more coherent and accurate summarization or generation, as the model has a clearer understanding of the subject matter.

### Trade-offs and Limitations of Entity-Based Chunking

While beneficial, Entity-Based Chunking also comes with trade-offs:

*   **Computational Cost**: NER is an NLP task that requires more computational resources than simple character or token splitting. Loading a spaCy model and processing the text multiple times (once for the whole document, then for each sentence) adds overhead, which can be significant for very large datasets.
*   **Dependency on NER Model Accuracy**: The effectiveness of this chunking method heavily relies on the accuracy of the underlying NER model. If the NER model fails to identify entities correctly or misses important ones, the chunks will not be optimally formed. Different NER models have varying performance across domains and entity types.
*   **Potential for Entity Sparsity**: Not all texts are rich in named entities. If a document discusses abstract concepts or general information without specific proper nouns, entity-based chunking might not yield significantly better results than other methods, or might even produce very large chunks if a single entity is mentioned throughout.
*   **Chunk Size Variability**: The size of entity-based chunks can vary significantly, from very small (a few sentences for a minor entity) to very large (many paragraphs for a major entity). This variability might be challenging for systems that expect uniformly sized chunks.
*   **Complexity in Implementation**: Implementing robust entity-based chunking can be more complex than simpler methods, requiring careful handling of entity linking, overlapping entities, and defining what constitutes a 'related' sentence for an entity.

## Explain Entity-Based Chunking

### Core Concept of Entity-Based Chunking

Entity-Based Chunking is a text splitting strategy that focuses on preserving the context around specific entities (persons, organizations, locations, dates, etc.) within a document. Instead of splitting text purely by character count or predefined delimiters, it leverages **Named Entity Recognition (NER)** to identify important entities. The goal is to group sentences or text segments that mention or are highly relevant to a particular entity together. This approach aims to create more semantically coherent chunks, making them more useful for retrieval tasks where the user's query might revolve around specific entities.

### Line-by-Line Explanation of Code in Cell `j9wi2xop129d`

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp(TEXT)
sent_texts = [s.text.strip() for s in doc.sents if s.text.strip()]
print(entity_map)
for ent in doc.ents:
  entity_map.setdefault(ent.text,set())
print(entity_map)
for s in sent_texts:
  s_doc = nlp(s)
  for ent in s_doc.ents:
    entity_map.setdefault(ent.text,set()).add(s)
entity_chunks=[]
for ent,ss in entity_map.items():
  entity_chunks.append(f"ENTITY :{ent}\n"+"\n".join(f"-{X}" for X in sorted(ss)))
entity_chunks=sent_texts
entity_chunks
```

1.  **`import spacy`**: This line imports the spaCy library, a popular open-source library for advanced Natural Language Processing (NLP) in Python.

2.  **`nlp = spacy.load("en_core_web_sm")`**: This loads a pre-trained English language model (`en_core_web_sm`) from spaCy. This model includes components for tokenization, part-of-speech tagging, dependency parsing, and, crucially for this task, **Named Entity Recognition (NER)**. It also provides capabilities for sentence segmentation.

3.  **`doc = nlp(TEXT)`**: The input `TEXT` is processed by the loaded spaCy `nlp` pipeline. This creates a `Doc` object, which contains linguistic annotations for the text, including identified entities and sentence boundaries.

4.  **`sent_texts = [s.text.strip() for s in doc.sents if s.text.strip()]`**: This line extracts individual sentences from the `doc` object. `doc.sents` is an iterator that yields `Span` objects representing sentences. For each sentence `s`, `s.text.strip()` gets its string content and removes leading/trailing whitespace. The list comprehension constructs `sent_texts`, a list of clean, individual sentences.

5.  **`entity_map = {}`**: An empty dictionary named `entity_map` is initialized. This dictionary is intended to store entities as keys and a set of sentences associated with each entity as values.

6.  **`for ent in doc.ents: entity_map.setdefault(ent.text,set())`**: This loop iterates through all the entities (`doc.ents`) that spaCy's NER component found in the entire `TEXT`. For each entity `ent`, its text (`ent.text`) is used as a key in `entity_map`. `setdefault(key, default_value)` ensures that if the key doesn't exist, it's added with an empty set as its value. This step initializes entries in `entity_map` for all unique entities found.

7.  **`for s in sent_texts: s_doc = nlp(s); for ent in s_doc.ents: entity_map.setdefault(ent.text,set()).add(s)`**: This nested loop is designed to associate sentences with the entities they contain. It iterates through each sentence in `sent_texts`. For each sentence `s`, it's re-processed by `nlp(s)` to create a new `Doc` object (`s_doc`). This is done to ensure entity detection is performed specifically on the sentence boundaries, which can sometimes differ from how entities are detected across the entire document. Then, it iterates through entities found within *that specific sentence* (`s_doc.ents`), and adds the current sentence `s` to the set associated with that entity in `entity_map`.

8.  **`entity_chunks=[]
for ent,ss in entity_map.items():
  entity_chunks.append(f"ENTITY :{ent}\n"+"\n".join(f"-{X}" for X in sorted(ss)))`**: This loop aims to construct the final `entity_chunks`. It iterates through the `entity_map` (key-value pairs of entity text and their associated sentences). For each entity, it formats a string that starts with "ENTITY :[entity_text]" followed by a newline and then lists all the sentences associated with that entity, each prefixed with a hyphen.

9.  **`entity_chunks=sent_texts`**: This line **overwrites** the `entity_chunks` list created in the previous step. It sets `entity_chunks` to be identical to the original list of `sent_texts`. This means the entity-based chunking logic's output, as initially intended in the previous loop, is effectively discarded in the final output of the cell.

#### Analysis of Observed Output

*   **`print(entity_map)` being empty**: After the initial `for ent in doc.ents: entity_map.setdefault(ent.text,set())` loop, `entity_map` is printed as `{}`. This indicates that the `en_core_web_sm` model did not detect any named entities in the specific `TEXT` provided: `'Artificial intellegence is transforming healthcare rapidly.Machine learning models can now detect desease from medical images with remarkable accuray.This technology promises to make diagnosis faster and more accesible worldwide.'`. The 'sm' (small) model might have limited entity recognition capabilities, or the text simply lacks entities that it's trained to recognize (like specific people, organizations, locations). Words like 'Artificial intellegence', 'healthcare', 'Machine learning models' are generally not classified as named entities by default 'en_core_web_sm' model, which focuses on proper nouns like names, places, dates, etc.

*   **`entity_chunks` being the same as `sent_texts`**: Due to the fact that no entities were detected, `entity_map` remains empty throughout the process. Consequently, the loop intended to construct `entity_chunks` based on `entity_map` would not add any items. However, the line `entity_chunks=sent_texts` explicitly assigns the list of raw sentences to `entity_chunks`, effectively bypassing any entity-based processing that might have occurred if entities were found.

### General Applications and Potential Benefits for LLMs and RAG Systems

Entity-Based Chunking offers several advantages for Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems:

*   **Improved Contextual Relevance**: By grouping text segments around key entities, RAG systems can retrieve more focused and contextually rich information when a query pertains to a specific entity. For example, if a query is about 'Apple Inc.', chunks related to 'Apple Inc.' as an organization, its products, and its history will be prioritized, ensuring all relevant details are present.
*   **Reduced Irrelevant Information**: Traditional fixed-size chunking might break an entity's context across multiple chunks or include unrelated information within a chunk. Entity-based chunking aims to minimize this by ensuring that all pertinent information about an entity resides within a single or closely related set of chunks.
*   **Enhanced Retrieval Accuracy**: When chunks are semantically aligned with entities, the embeddings generated for these chunks can better capture the entity's meaning. This leads to more accurate retrieval of relevant documents or passages, as the similarity between the query's embedding and the chunk's embedding will be higher for entity-centric queries.
*   **Better Summarization and Generation**: For LLMs, having well-formed, entity-specific chunks as context can lead to more coherent and accurate summarization or generation, as the model has a clearer understanding of the subject matter.

### Trade-offs and Limitations of Entity-Based Chunking

While beneficial, Entity-Based Chunking also comes with trade-offs:

*   **Computational Cost**: NER is an NLP task that requires more computational resources than simple character or token splitting. Loading a spaCy model and processing the text multiple times (once for the whole document, then for each sentence) adds overhead, which can be significant for very large datasets.
*   **Dependency on NER Model Accuracy**: The effectiveness of this chunking method heavily relies on the accuracy of the underlying NER model. If the NER model fails to identify entities correctly or misses important ones, the chunks will not be optimally formed. Different NER models have varying performance across domains and entity types.
*   **Potential for Entity Sparsity**: Not all texts are rich in named entities. If a document discusses abstract concepts or general information without specific proper nouns, entity-based chunking might not yield significantly better results than other methods, or might even produce very large chunks if a single entity is mentioned throughout.
*   **Chunk Size Variability**: The size of entity-based chunks can vary significantly, from very small (a few sentences for a minor entity) to very large (many paragraphs for a major entity). This variability might be challenging for systems that expect uniformly sized chunks.
*   **Complexity in Implementation**: Implementing robust entity-based chunking can be more complex than simpler methods, requiring careful handling of entity linking, overlapping entities, and defining what constitutes a 'related' sentence for an entity.

## Final Task

### Subtask:
Summarize the key takeaways for an AI Architecture/ML Engineering student regarding text splitting and its practical applications.

## Summary:

### Q&A

**What are the key takeaways for an AI Architecture/ML Engineering student regarding text splitting and its practical applications?**

For AI Architecture/ML Engineering students, understanding text splitting is crucial for optimizing Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems. Key takeaways include:

*   **Context Window Management is Paramount**: LLMs have strict token limits. Employing token-based splitters like `TokenTextSplitter` with the correct `encoding_name` (e.g., `cl100k_base`) is essential to prevent input truncation and manage API costs efficiently.
*   **Semantic Coherence vs. Computational Cost**: There's a trade-off between semantic coherence and computational expense. Simple splitters (e.g., `CharacterTextSplitter`) are fast but lack semantic awareness. Advanced methods like `SpacyTextSplitter`, `SemanticChunker`, and `Entity-Based Chunking` offer greater linguistic accuracy and semantic integrity, but incur higher computational overhead due to dependency on NLP models or embedding generation.
*   **Recursive Splitting as a General-Purpose Solution**: `RecursiveCharacterTextSplitter` is often the default choice due to its robustness. It balances semantic awareness by attempting to split on larger delimiters first (e.g., paragraphs, sentences) before resorting to character-level splits, while also managing chunk size and overlap effectively.
*   **Overlap is Critical for Context**: Regardless of the splitter, utilizing `chunk_overlap` (e.g., 10-20% of `chunk_size`) is vital to maintain continuity and prevent context loss when information spans across chunk boundaries, improving the LLM's understanding and response quality.
*   **Domain-Specific Considerations**: The optimal splitting strategy depends on the domain and task. For highly structured text or code, simple character or recursive splitting might suffice. For rich, narrative text or specific entity-focused queries, `SpacyTextSplitter`, `SemanticChunker`, or `Entity-Based Chunking` (if entities are prominent) will yield better results, despite their higher complexity and resource demands. Always evaluate the chosen splitter's performance on your specific dataset.

### Data Analysis Key Findings

*   **Initial Setup**: The notebook required installations of `langchain-text-splitters`, `langchain_experimental`, `spacy`, `tiktoken`, and `sentence-transformers` for various splitting techniques, and `spaCy`'s `en_core_web_sm` model for linguistic processing. The `TEXT` variable served as a consistent sample for all demonstrations.
*   **`CharacterTextSplitter`**:
    *   By default, it splits on `\n\n`. If this separator is absent and no `chunk_size` is specified, the entire text remains a single chunk.
    *   Configured with `chunk_size=10` and `separator=''`, it performs character-level splitting, producing chunks of exactly 10 characters, irrespective of semantic boundaries.
    *   This splitter is basic, fast, but lacks semantic coherence, often breaking words or sentences.
*   **`RecursiveCharacterTextSplitter`**:
    *   Utilizes a hierarchical list of separators (e.g., `\n\n`, `\n`, ` `) to break text, prioritizing larger, more meaningful units.
    *   Demonstrated splitting `TEXT` into chunks of approximately 100 characters with a `chunk_overlap` of 20 characters, effectively preserving context across chunks.
    *   Offers a good balance between semantic coherence and robustness for general-purpose text splitting.
*   **`TokenTextSplitter`**:
    *   Splits text based on token counts using a specified encoding like `cl100k_base` (for OpenAI models), directly addressing LLM context window limitations.
    *   Demonstrated splitting `TEXT` into chunks of approximately 20 tokens with a 10-token overlap, ensuring precise token management and contextual flow.
    *   Crucial for managing LLM input limits and optimizing API costs.
*   **`SpacyTextSplitter`**:
    *   Leverages `spaCy`'s NLP capabilities (`en_core_web_sm` model) for accurate sentence boundary detection, producing linguistically coherent sentence-level chunks.
    *   The output format for the sample `TEXT` showed a single string with `\n\n` separators, indicating it preserves internal text structure while identifying sentences.
    *   Provides high linguistic accuracy but incurs higher computational cost due to `spaCy` model loading and processing.
*   **`SemanticChunker`**:
    *   Splits text based on semantic similarity using embedding models (e.g., `HuggingFaceEmbeddings` with `all-MiniLM-L6-v2`) to identify topic shifts.
    *   For the provided short and semantically coherent `TEXT`, the `SemanticChunker` returned it as a single chunk, as no significant semantic boundaries were detected.
    *   Offers superior semantic coherence for complex documents but is computationally intensive due to embedding generation.
*   **`Entity-Based Chunking`**:
    *   Aims to group sentences around specific named entities identified by `spaCy`'s Named Entity Recognition (NER).
    *   The `en_core_web_sm` model did not detect any named entities in the sample `TEXT` (e.g., "Artificial intelligence" was not recognized as a standard entity type). This resulted in an empty `entity_map`.
    *   A subsequent line `entity_chunks=sent_texts` effectively overwrote the intended entity-based output, making the final result a list of raw sentences.
    *   Despite the sample text limitation, the concept holds potential for highly targeted RAG and information retrieval for entity-centric queries but requires robust NER models and is computationally more expensive.

### Insights or Next Steps

*   **Experimentation with Parameters**: The effectiveness of text splitting is highly sensitive to `chunk_size`, `chunk_overlap`, and the chosen splitter. Engineers should perform systematic experimentation with these parameters and evaluate their impact on downstream tasks (e.g., RAG retrieval accuracy, LLM response quality) for their specific datasets.
*   **Hybrid Approaches**: Consider combining different splitting strategies. For instance, an initial `RecursiveCharacterTextSplitter` pass for coarse-grained chunking, followed by a `SpacyTextSplitter` or `SemanticChunker` on the larger chunks for fine-grained, semantically aware sub-chunking, could yield optimal results for complex documents.

## Summary of Text Splitting Techniques for LLMs and RAG

This notebook explored various **text splitting (chunking) techniques** crucial for preparing long documents for Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems. The core challenge is to break down text into smaller, manageable chunks that fit within an LLM's context window while preserving semantic coherence.

We examined and demonstrated the following methods:

1.  **`CharacterTextSplitter`**: A basic splitter that divides text based on specified characters (e.g., `\n\n`) or fixed character counts. It's simple and fast but often lacks semantic awareness.
2.  **`RecursiveCharacterTextSplitter`**: A robust and flexible approach that recursively attempts to split text using a list of separators (e.g., paragraphs, sentences, words) until chunks are within a target size, often including a `chunk_overlap` to maintain context.
3.  **`TokenTextSplitter`**: Essential for LLMs, this splitter uses tokenization (e.g., `tiktoken` for OpenAI models) to create chunks based on token counts, directly managing the LLM's context window and optimizing costs.
4.  **`SpacyTextSplitter`**: Leverages spaCy's advanced NLP capabilities to perform linguistically accurate sentence-level splitting, ensuring semantic coherence by respecting natural sentence boundaries.
5.  **`SemanticChunker`**: An advanced technique that uses embedding models (e.g., `HuggingFaceEmbeddings`) to identify and split text at points where the semantic meaning significantly changes, resulting in highly coherent, topic-focused chunks.
6.  **`Entity-Based Chunking`**: A conceptual approach (demonstrated with spaCy NER) that aims to group sentences around specific named entities. This is ideal for RAG systems where queries are entity-centric, though it requires robust NER and can be computationally intensive.

### Key Takeaways for AI/ML Engineering Students:

*   **Context Management**: Always prioritize managing the LLM's context window, especially with token-based splitters.
*   **Balance Trade-offs**: Choose a splitter by balancing semantic coherence, computational cost, and implementation complexity based on your specific task and data.
*   **Overlap is Critical**: Use `chunk_overlap` to prevent context loss across chunk boundaries, improving LLM understanding.
*   **Domain-Specific Strategies**: The best splitting method depends on the text's nature. More sophisticated methods offer higher quality but come with increased resource demands.

Understanding these techniques is vital for building effective and efficient LLM and RAG applications, ensuring that models receive the most relevant and coherent information possible.
"""